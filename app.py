import streamlit as st
from langchain_classic.chains import create_history_aware_retriever, create_retrieval_chain
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langchain_ollama import OllamaEmbeddings
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
import os
import tempfile
import requests

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Customer Support RAG Chatbot",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Customer Support RAG Chatbot (100% –ª–æ–∫–∞–ª—å–Ω—ã–π)")
st.markdown("---")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.chat_history = ChatMessageHistory()
    st.session_state.vectorstore_initialized = False

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama
def check_ollama():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        return response.status_code == 200
    except:
        return False

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama Chat
@st.cache_resource
def init_ollama_chat():
    try:
        llm = ChatOllama(
            model="llama3",  # –ú–æ–¥–µ–ª—å –¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤
            temperature=0.7,
            num_predict=2048,
            top_k=10,
            top_p=0.95,
            num_ctx=4096
        )
        return llm
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChatOllama: {str(e)}")
        return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Ollama Embeddings
@st.cache_resource
def init_ollama_embeddings():
    try:
        embeddings = OllamaEmbeddings(
            model="nomic-embed-text",  # –°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
        )
        return embeddings
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OllamaEmbeddings: {str(e)}")
        return None

# –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
if not check_ollama():
    st.error("""
    ‚ö†Ô∏è **Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞!**
    
    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:
    1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: `ollama serve` (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)
    2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª–∏: 
       - `ollama pull llama3` (–¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤)
       - `ollama pull nomic-embed-text` (–¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
    3. –û–±–Ω–æ–≤–∏—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
    """)
    st.stop()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
llm = init_ollama_chat()
embeddings = init_ollama_embeddings()

if llm is None or embeddings is None:
    st.stop()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
    st.subheader("ü§ñ –ú–æ–¥–µ–ª–∏ Ollama")
    st.info("""
    **Chat model:** llama3 (–¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤)
    **Embeddings model:** nomic-embed-text (–¥–ª—è –ø–æ–∏—Å–∫–∞)
    
    ‚úÖ –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–º–±–∏–Ω–∞—Ü–∏—è!
    """)
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π",
        type="pdf"
    )
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = ChatMessageHistory()
            st.rerun()
    
    with col2:
        if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –ë–î", use_container_width=True):
            import shutil
            if os.path.exists("./chroma_db_ollama"):
                shutil.rmtree("./chroma_db_ollama")
            st.session_state.vectorstore_initialized = False
            st.rerun()
    
    st.markdown("---")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    if st.button("üìã –ü–æ–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª–∏ Ollama", use_container_width=True):
        try:
            response = requests.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                st.write("**–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:**")
                for model in models:
                    st.write(f"- {model['name']} ({model['size'] / 1e9:.1f} GB)")
        except:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
if uploaded_file is not None and not st.session_state.vectorstore_initialized:
    with st.status("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞...", expanded=True) as status:
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            st.write("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ PDF...")
            loader = PyPDFLoader(tmp_file_path)
            documents = loader.load()
            st.write(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks
            st.write("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", " ", ""]
            )
            splits = text_splitter.split_documents(documents)
            st.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(splits)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            # –°–û–ó–î–ê–ï–ú –í–ï–ö–¢–û–†–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï –° NOMIC-EMBED-TEXT
            st.write("üíæ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å nomic-embed-text...")
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory="./chroma_db_ollama"
            )
            
            retriever = vectorstore.as_retriever(
                search_type="similarity",
                search_kwargs={"k": 4}  # –í–æ–∑–≤—Ä–∞—â–∞–µ–º 4 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
            )
            
            # Prompt –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
            contextualize_q_prompt = ChatPromptTemplate.from_messages([
                ("system", """Given a chat history and the latest user question 
                which might reference context in the chat history, formulate a standalone question 
                which can be understood without the chat history. Do NOT answer the question, 
                just reformulate it if needed and otherwise return it as is."""),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ])
            
            history_aware_retriever = create_history_aware_retriever(
                llm, retriever, contextualize_q_prompt
            )
            
            # Prompt –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã
            system_prompt = """You are a helpful customer support assistant. 
            Use the following pieces of retrieved context to answer the user's question.
            If you don't know the answer based on the context, say that you don't know.
            Be concise, friendly, and professional. Show empathy when users express frustration.
            
            Context: {context}"""
            
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ])
            
            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
            
            st.session_state.conversational_rag_chain = RunnableWithMessageHistory(
                rag_chain,
                lambda: st.session_state.chat_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer"
            )
            
            st.session_state.vectorstore_initialized = True
            os.unlink(tmp_file_path)
            
            status.update(label="‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å nomic-embed-text!", state="complete")
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            st.exception(e)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∞—Ç–∞
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
if not st.session_state.messages:
    with st.chat_message("assistant"):
        st.markdown("""
        üëã **–ü—Ä–∏–≤–µ—Ç! –Ø –ø–æ–ª–Ω–æ—Å—Ç—å—é –ª–æ–∫–∞–ª—å–Ω—ã–π –±–æ—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏ –Ω–∞ Ollama!**
        
        **üîß –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
        - üí¨ **Chat model:** `llama3` (–¥–ª—è –æ—Ç–≤–µ—Ç–æ–≤)
        - üîç **Embeddings model:** `nomic-embed-text` (–¥–ª—è –ø–æ–∏—Å–∫–∞)
        - üìä **–ü–æ–∏—Å–∫:** 4 –Ω–∞–∏–±–æ–ª–µ–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞
        - üíæ **–•—Ä–∞–Ω–∏–ª–∏—â–µ:** ChromaDB
        
        **üìù –ß—Ç–æ —è —É–º–µ—é:**
        - –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ PDF
        - –ü–æ–º–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        - –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –≤–æ–ø—Ä–æ—Å–æ–≤
        
        **‚¨ÖÔ∏è –ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF** –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å!
        """)

# –ü–æ–ª–µ –≤–≤–æ–¥–∞
if st.session_state.vectorstore_initialized:
    user_input = st.chat_input("üí¨ –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å...")
    
    if user_input:
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        with st.chat_message("assistant"):
            with st.spinner("ü§î –î—É–º–∞—é..."):
                try:
                    response = st.session_state.conversational_rag_chain.invoke(
                        {"input": user_input}
                    )
                    
                    bot_response = response['answer']
                    st.markdown(bot_response)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                    show_context = st.sidebar.checkbox("üìö –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", False)
                    if show_context and 'context' in response:
                        with st.expander("üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"):
                            for i, doc in enumerate(response['context']):
                                st.markdown(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}:**")
                                st.info(doc.page_content)
                                st.markdown("---")
                    
                except Exception as e:
                    bot_response = f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"
                    st.error(bot_response)
                    st.exception(e)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        st.session_state.messages.append({"role": "assistant", "content": bot_response})

else:
    # –û—Ç–∫–ª—é—á–∞–µ–º –≤–≤–æ–¥, –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω PDF
    st.chat_input("üí¨ –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ PDF –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏...", disabled=True)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É
    if not uploaded_file:
        col1, col2, col3 = st.columns(3)
        with col2:
            st.info("üëà –ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ñ–∞–π–ª –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—â–µ–Ω–∏–µ")