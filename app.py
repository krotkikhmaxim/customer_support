import streamlit as st
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_ollama import ChatOllama
from langchain_huggingface import HuggingFaceEmbeddings  # –î–ª—è multilingual-e5-large
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
import os
import tempfile
import requests
import torch

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
st.set_page_config(
    page_title="Customer Support RAG Chatbot",
    page_icon="ü§ñ",
    layout="wide"
)

st.title("ü§ñ Customer Support RAG Chatbot (100% –ª–æ–∫–∞–ª—å–Ω—ã–π)")
st.markdown("---")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏
if "messages" not in st.session_state:
    st.session_state.messages = []
    st.session_state.chat_history = ChatMessageHistory()
    st.session_state.vectorstore_initialized = False

# –ü—Ä–æ–≤–µ—Ä–∫–∞ Ollama
def check_ollama():
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=2)
        return response.status_code == 200
    except:
        return False

@st.cache_resource
def init_ollama_chat():
    try:
        llm = ChatOllama(
            model="gpt-oss:20b",
            temperature=0.7,
            num_predict=2048,
            top_k=40,
            top_p=0.9,
            num_ctx=8192,
            repeat_penalty=1.1,
            format="json",  # –î–ª—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        )
        return llm
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ ChatOllama: {str(e)}")
        return None

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è E5-large embeddings (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–µ)
@st.cache_resource
def init_e5_embeddings():
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ CUDA –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è
        device = "cuda" if torch.cuda.is_available() else "cpu"
        if device == "cuda":
            st.sidebar.success(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è GPU: {torch.cuda.get_device_name(0)}")
        else:
            st.sidebar.info("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è GPU –¥–ª—è E5-large)")
        
        embeddings = HuggingFaceEmbeddings(
            model_name="intfloat/multilingual-e5-large",
            model_kwargs={
                'device': device,
                'torch_dtype': torch.float16 if device == "cuda" else torch.float32
            },
            encode_kwargs={
                'normalize_embeddings': True,
                'batch_size': 32  # –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            }
        )
        return embeddings
    except Exception as e:
        st.error(f"–û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ E5 embeddings: {str(e)}")
        return None

# –§—É–Ω–∫—Ü–∏—è –¥–ª—è —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º –¥–ª—è E5
def prepare_text_for_e5(text, is_query=False):
    """E5 —Ç—Ä–µ–±—É–µ—Ç —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∏ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if is_query:
        return f"query: {text}"
    else:
        return f"passage: {text}"

# –ü—Ä–æ–≤–µ—Ä—è–µ–º Ollama
if not check_ollama():
    st.error("""
    ‚ö†Ô∏è **Ollama –Ω–µ –∑–∞–ø—É—â–µ–Ω–∞!**
    
    –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:
    1. –ó–∞–ø—É—Å—Ç–∏—Ç–µ Ollama: `ollama serve` (–≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º —Ç–µ—Ä–º–∏–Ω–∞–ª–µ)
    2. –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: `ollama pull gpt-oss:20b`
    3. –û–±–Ω–æ–≤–∏—Ç–µ —ç—Ç—É —Å—Ç—Ä–∞–Ω–∏—Ü—É
    """)
    st.stop()

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –º–æ–¥–µ–ª–∏
llm = init_ollama_chat()
embeddings = init_e5_embeddings()

if llm is None or embeddings is None:
    st.stop()

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å
with st.sidebar:
    st.header("üìÅ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª—è—Ö
    st.subheader("ü§ñ –ú–æ–¥–µ–ª–∏")
    st.info("""
    **Chat model:** `gpt-oss:20b` (20B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
    **Embeddings:** `intfloat/multilingual-e5-large` (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω–∞—è)
    
    üåç –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —è–∑—ã–∫–æ–≤: 100+ —è–∑—ã–∫–æ–≤
    üìä –†–∞–∑–º–µ—Ä –º–æ–¥–µ–ª–∏: ~2.2 GB
    """)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    st.subheader("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –ø–æ–∏—Å–∫–∞")
    k_results = st.slider("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞", min_value=2, max_value=10, value=5)
    chunk_size = st.slider("–†–∞–∑–º–µ—Ä —Ñ—Ä–∞–≥–º–µ–Ω—Ç–∞ (—Ç–æ–∫–µ–Ω–æ–≤)", min_value=500, max_value=2000, value=1000, step=100)
    chunk_overlap = st.slider("–ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤", min_value=0, max_value=500, value=200, step=50)
    
    st.markdown("---")
    
    uploaded_file = st.file_uploader(
        "–í—ã–±–µ—Ä–∏—Ç–µ PDF —Ñ–∞–π–ª —Å –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π",
        type="pdf"
    )
    
    st.markdown("---")
    
    col1, col2 = st.columns(2)
    with col1:
        if st.button("üóëÔ∏è –û—á–∏—Å—Ç–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é", use_container_width=True):
            st.session_state.messages = []
            st.session_state.chat_history = ChatMessageHistory()
            st.rerun()
    
    with col2:
        if st.button("üîÑ –°–±—Ä–æ—Å–∏—Ç—å –ë–î", use_container_width=True):
            import shutil
            if os.path.exists("./chroma_db_e5"):
                shutil.rmtree("./chroma_db_e5")
            st.session_state.vectorstore_initialized = False
            st.rerun()
    
    st.markdown("---")
    
    # –ü–æ–∫–∞–∑–∞—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏
    if st.button("üìã –ü–æ–∫–∞–∑–∞—Ç—å –º–æ–¥–µ–ª–∏ Ollama", use_container_width=True):
        try:
            response = requests.get("http://localhost:11434/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                st.write("**–î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏:**")
                for model in models:
                    st.write(f"- {model['name']} ({model['size'] / 1e9:.1f} GB)")
        except:
            st.error("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å–ø–∏—Å–æ–∫ –º–æ–¥–µ–ª–µ–π")

# –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
if uploaded_file is not None and not st.session_state.vectorstore_initialized:
    with st.status("üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–∞...", expanded=True) as status:
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf') as tmp_file:
                tmp_file.write(uploaded_file.getvalue())
                tmp_file_path = tmp_file.name
            
            st.write("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ PDF...")
            loader = PyPDFLoader(tmp_file_path)
            documents = loader.load()
            st.write(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(documents)} —Å—Ç—Ä–∞–Ω–∏—Ü")
            
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ chunks —Å —É—á–µ—Ç–æ–º –Ω–∞—Å—Ç—Ä–æ–µ–∫
            st.write("‚úÇÔ∏è –†–∞–∑–±–∏–≤–∫–∞ –Ω–∞ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã...")
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                separators=["\n\n", "\n", ".", "!", "?", ",", " ", ""],
                length_function=len,
            )
            splits = text_splitter.split_documents(documents)
            st.write(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ {len(splits)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤")
            
            # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å—ã –¥–ª—è E5 –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
            st.write("üîß –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–ª—è E5 embeddings...")
            for doc in splits:
                doc.page_content = prepare_text_for_e5(doc.page_content, is_query=False)
            
            # –°–û–ó–î–ê–ï–ú –í–ï–ö–¢–û–†–ù–û–ï –•–†–ê–ù–ò–õ–ò–©–ï –° E5-large
            st.write("üíæ –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö —Å multilingual-e5-large...")
            vectorstore = Chroma.from_documents(
                documents=splits,
                embedding=embeddings,
                persist_directory="./chroma_db_e5",
                collection_metadata={"hnsw:space": "cosine"}  # E5 –∏—Å–ø–æ–ª—å–∑—É–µ—Ç cosine similarity
            )
            
            # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º
            class E5Retriever:
                def __init__(self, vectorstore, k=5):
                    self.vectorstore = vectorstore
                    self.k = k
                
                def get_relevant_documents(self, query):
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞
                    query_with_prefix = prepare_text_for_e5(query, is_query=True)
                    return self.vectorstore.similarity_search(query_with_prefix, k=self.k)
            
            retriever = E5Retriever(vectorstore, k=k_results)
            
            # Prompt –¥–ª—è –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞–Ω–∏—è –≤–æ–ø—Ä–æ—Å–∞ —Å —É—á–µ—Ç–æ–º –∏—Å—Ç–æ—Ä–∏–∏
            contextualize_q_prompt = ChatPromptTemplate.from_messages([
                ("system", """Given a chat history and the latest user question 
                which might reference context in the chat history, formulate a standalone question 
                which can be understood without the chat history. Do NOT answer the question, 
                just reformulate it if needed and otherwise return it as is.
                
                Important: The question might be in any language. Preserve the original language."""),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ])
            
            history_aware_retriever = create_history_aware_retriever(
                llm, retriever, contextualize_q_prompt
            )
            
            # Prompt –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã (–º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π)
            system_prompt = """You are a helpful customer support assistant. You can communicate in multiple languages.
            Use the following pieces of retrieved context to answer the user's question.
            If you don't know the answer based on the context, say that you don't know.
            Be concise, friendly, and professional. Show empathy when users express frustration.
            
            Important: Answer in the SAME LANGUAGE as the user's question.
            
            Context: {context}"""
            
            qa_prompt = ChatPromptTemplate.from_messages([
                ("system", system_prompt),
                MessagesPlaceholder("chat_history"),
                ("human", "{input}"),
            ])
            
            question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
            rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)
            
            st.session_state.conversational_rag_chain = RunnableWithMessageHistory(
                rag_chain,
                lambda session_id: st.session_state.chat_history,
                input_messages_key="input",
                history_messages_key="chat_history",
                output_messages_key="answer"
            )
            
            st.session_state.vectorstore_initialized = True
            os.unlink(tmp_file_path)
            
            status.update(label="‚úÖ –î–æ–∫—É–º–µ–Ω—Ç —É—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω —Å multilingual-e5-large!", state="complete")
            
        except Exception as e:
            st.error(f"‚ùå –û—à–∏–±–∫–∞: {str(e)}")
            st.exception(e)

# –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —á–∞—Ç–∞
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# –ü—Ä–∏–≤–µ—Ç—Å—Ç–≤–µ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
if not st.session_state.messages:
    with st.chat_message("assistant"):
        st.markdown("""
        üëã **–ü—Ä–∏–≤–µ—Ç! –Ø –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π –±–æ—Ç —Å–ª—É–∂–±—ã –ø–æ–¥–¥–µ—Ä–∂–∫–∏!**
        
        **üîß –¢–µ–∫—É—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:**
        - üí¨ **Chat model:** `gpt-oss:20b` (20B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
        - üîç **Embeddings model:** `intfloat/multilingual-e5-large` (2.2GB)
        - üåç **–ü–æ–¥–¥–µ—Ä–∂–∫–∞:** 100+ —è–∑—ã–∫–æ–≤
        - üìä **–ü–æ–∏—Å–∫:** –Ω–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤
        - üíæ **–•—Ä–∞–Ω–∏–ª–∏—â–µ:** ChromaDB —Å cosine similarity
        
        **üìù –ß—Ç–æ —è —É–º–µ—é:**
        - –û—Ç–≤–µ—á–∞—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ
        - –ü–æ–Ω–∏–º–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —è–∑—ã–∫–∞—Ö
        - –†–∞–±–æ—Ç–∞—Ç—å —Å –±–æ–ª—å—à–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏
        - –ü–æ–º–Ω–∏—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
        
        **‚¨ÖÔ∏è –ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF** –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å!
        """)

# –ü–æ–ª–µ –≤–≤–æ–¥–∞
if st.session_state.vectorstore_initialized:
    user_input = st.chat_input("üí¨ –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ...")
    
    if user_input:
        # –î–æ–±–∞–≤–ª—è–µ–º —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        st.session_state.messages.append({"role": "user", "content": user_input})
        with st.chat_message("user"):
            st.markdown(user_input)
        
        # –ü–æ–ª—É—á–∞–µ–º –æ—Ç–≤–µ—Ç
        with st.chat_message("assistant"):
            with st.spinner("ü§î –î—É–º–∞—é..."):
                try:
                    response = st.session_state.conversational_rag_chain.invoke(
                        {"input": user_input},
                        config={"configurable": {"session_id": "default"}}
                    )
                    
                    bot_response = response['answer']
                    st.markdown(bot_response)
                    
                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
                    show_context = st.sidebar.checkbox("üìö –ü–æ–∫–∞–∑–∞—Ç—å –∫–æ–Ω—Ç–µ–∫—Å—Ç", False)
                    if show_context and 'context' in response:
                        with st.expander("üìö –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–µ —Ñ—Ä–∞–≥–º–µ–Ω—Ç—ã"):
                            for i, doc in enumerate(response['context']):
                                # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ—Ñ–∏–∫—Å –¥–ª—è –æ—Ç–æ–±—Ä–∞–∂–µ–Ω–∏—è
                                content = doc.page_content.replace("passage: ", "")
                                st.markdown(f"**–§—Ä–∞–≥–º–µ–Ω—Ç {i+1}:**")
                                st.info(content)
                                st.markdown("---")
                    
                except Exception as e:
                    bot_response = f"‚ùå –û—à–∏–±–∫–∞: {str(e)}"
                    st.error(bot_response)
                    st.exception(e)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        st.session_state.messages.append({"role": "assistant", "content": bot_response})

else:
    # –û—Ç–∫–ª—é—á–∞–µ–º –≤–≤–æ–¥, –ø–æ–∫–∞ –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω PDF
    st.chat_input("üí¨ –°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ PDF –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏...", disabled=True)
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–æ–¥—Å–∫–∞–∑–∫—É
    if not uploaded_file:
        col1, col2, col3 = st.columns(3)
        with col2:
            st.info("üëà –ó–∞–≥—Ä—É–∑–∏—Ç–µ PDF —Ñ–∞–π–ª –≤ –±–æ–∫–æ–≤–æ–π –ø–∞–Ω–µ–ª–∏, —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å –æ–±—â–µ–Ω–∏–µ")
